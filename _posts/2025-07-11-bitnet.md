---
layout: post
author: Dario Tortorici
title: "Bitnet: How 1-Bit LLMs Could Make AI Accessible for Everyone"
excerpt: "The promising frontier of 1-bit LLMs; a smarter, smaller, and more accessible AI. <br/>"
image: /assets/images/blog_thumbnails/bitnet.png
date: 2025-07-11
categories: 
  - AI
tags:
  - AI-research

---

# TLDR

BitNet is a highly efficient LLM architecture that uses just -1, 0, 1 as weights, drastically reducing memory and energy use during inference. This approach opens the door to running capable models locally, even on CPUs.

---

# Introduction
Recently, we discussed the problems with the architectures and reasoning behind current models. Today, I would like to continue this series of discussions by talking about the energy consumption and democratization of AI.

There are many open-source models available, including the excellent DeepSeek. However, running something like DeepSeek V3 on your own hardware is unrealistic. You can distill or shrink it, but it would still cost you a significant performance trade-off or require industrial GPUs.

Since the trend seems to be embedding intelligence everywhere, this is a problem. I don't think LLMs are the best architecture for developing everything, but they are the most promising at the moment, so we will work with them.

---

# Quantization

Quantization reduces the number of bits used to represent each model weight. While standard models use **FP16 (16-bit floats)**, quantized versions use **FP8** or less representations. This means that we can represent the parameters values with less thicks. FP16 gives fine-grained control (±0.001), whereas INT4 offers coarse steps (±1), impacting performance.

To counter information loss, it’s paired with calibration datasets and clever fine-tuning to bring performance back. In many cases, this tweaks allows FP8 model to outperform a smaller full-precision model.

# Bitnet
Microsoft researchers took it even further with [Bitnet](https://arxiv.org/pdf/2504.12285). They used just one bit per weight, representing either +1 or -1. This results in a memory footprint that is up to 16 times smaller, as well as the elimination of matrix multiplication. These properties are amazing because allow us to consume up to 30 times less energy in certain configurations.

Our minds work in a 0/1 way, more or less. Neurons fire or not. However, Bitnet uses -1 because it is more expressive than 0, allowing us to negate input characteristics. Sometimes, however, doing nothing is the smart move. This insight led to Bitnet B1.58, which adds a third weight state: 0. This introduces sparsity, enabling the model to deactivate certain connections dynamically and improving performance. Why "1.58"? It's the average number of bits needed to store three weight states, mathematically approximated. But how do you store 1.58 bits on real silicon? We can't with one bit. Since there is no native 1.58-bit data type, researchers use creative bit-packing tricks that allow them to achieve 1.67 bits per weight. This is very close to the theoretical minimum. However, perhaps the future will require new hardware to further exploit the potential of this architecture.

# Training Bitnet

This is where it gets tricky. Despite BitNet's impressive efficiency during inference, the training process is not more efficient. In fact, it’s more complex. Training still requires full precision, and in the final 10–20%, Quantization-Aware Training (QAT) kicks in to fine-tune the model for its compressed form. This adds an extra layer of complexity. You must keep a full-precision "master copy" of the weights throughout the process. Then, during each forward pass, these weights are temporarily quantized to ternary values (+1, -1, or 0). After the forward pass, the gradients are backpropagated into the full-precision master copy.

A further issue is that quantization requires rounding, which isn't differentiable. To work around this, we use a trick called the Straight-Through Estimator (STE). The STE pretends that the rounding function has a derivative of 1, which isn't mathematically correct but works well enough in practice. The catch is that STE is less stable and less efficient than regular backpropagation. Consequently, more training tricks, careful tuning, or extra epochs are often needed to reach comparable performance.

# Conclusion

Microsoft has open-sourced a 2 billion parameter BitNet model that you can [try out on Hugging Face](https://huggingface.co/microsoft/bitnet-b1.58-2B-4T) right now. Will it amaze you? No, BitNet isn't GPT-4. And it’s not trying to be. However, it's six times smaller than comparable models in the same parameter range, and it runs on a CPU. This is crucial because only if we can run local LLMs with privacy, we will have real control.

This kind of research reminds us that we’re not done yet. Before we talk about AGI, we have to address these architectural refinements. We can't continue using LLMs that require data center–grade GPUs just to generate a sentence. We can't infinitely scale the actual transformers. The future of AI is bigger, smarter, leaner, and more democratic.

---
Image Credits: Generated with AI (Dall-E 3)
